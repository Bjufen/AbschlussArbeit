{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The Dataset contains the following 12 features:\n",
    "\n",
    "CustomerID: A unique identifier\n",
    "\n",
    "Age: The age of the customer\n",
    "\n",
    "Gender: The gender of the customer\n",
    "\n",
    "Tenure: The number of months the customer has stayed with the company\n",
    "\n",
    "Usage Frequency: The number of times the customer has used the service the past month\n",
    "\n",
    "Support calls: The number of support calls the customer has made the past month\n",
    "\n",
    "Payment Delay: Number of days the customer has delayed payment the past month\n",
    "\n",
    "Subscription Type: The type of subscription the customer has\n",
    "\n",
    "Contract Length: Duration of the contract\n",
    "\n",
    "Total Spend: The total amount the customer has spent\n",
    "\n",
    "Last Interaction: Number of days since the last interaction the customer has had with the company\n",
    "\n",
    "Churn: Whether the customer has churned or not"
   ],
   "id": "9d1fbd99fdf5df0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import packages",
   "id": "eb52de71c88559da"
  },
  {
   "cell_type": "code",
   "id": "af274228a76fce1a",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "from sklearn.tree import DecisionTreeRegressor\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cleanup",
   "id": "e5aa3d024b73091a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_set_dirty = pd.read_csv(\"Datasets/In/customer_churn_dataset-testing-master.csv\", sep=\",\")\n",
    "training_set_dirty = pd.read_csv(\"Datasets/In/customer_churn_dataset-training-master.csv\", sep=\",\")\n",
    "\n",
    "combined_set_dirty = pd.concat([training_set_dirty, test_set_dirty], ignore_index=True)\n",
    "combined_set_dirty = combined_set_dirty.drop(combined_set_dirty.columns[0], axis=1)\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "missing_values = combined_set_dirty.isnull().sum()\n",
    "missing_values"
   ],
   "id": "3a5f283dfd9c40ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "combined_set_dirty[combined_set_dirty.isna().any(axis=1)]",
   "id": "7291a6e82abe9500",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# remove row with missing values\n",
    "combined_set_dirty.dropna(inplace=True)\n",
    "\n",
    "combined_set_dirty.columns = [col.lower().replace(\" \", \"_\") for col in combined_set_dirty.columns]\n",
    "combined_set_dirty.info()"
   ],
   "id": "521699c3f844d8dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "combined_set_dirty[combined_set_dirty.isna().any(axis=1)]",
   "id": "386edb816ee447ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numerals = [\"age\", \"tenure\", \"usage_frequency\", \"support_calls\", \"payment_delay\", \"last_interaction\", \"churn\"]\n",
    "\n",
    "for col in numerals:\n",
    "    combined_set_dirty[col] = combined_set_dirty[col].astype(int)\n",
    "    "
   ],
   "id": "7a7258d15a4fea7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Create a label encoder object\n",
    "# le = LabelEncoder()\n",
    "# \n",
    "# # List of columns you want to convert\n",
    "# columns_to_convert = ['gender', 'subscription_type', 'contract_length']\n",
    "# \n",
    "# # Apply the label encoder to each column and print the mapping\n",
    "# for column in columns_to_convert:\n",
    "#     combined_set_dirty[column] = le.fit_transform(combined_set_dirty[column])\n",
    "#     print(f\"Mapping for {column}:\")\n",
    "#     for class_, label in zip(le.classes_, range(len(le.classes_))):\n",
    "#         print(f\"{class_} -> {label}\")\n",
    "#     print(\"\\n\")"
   ],
   "id": "9e308f344b590636",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cleaned_set = combined_set_dirty.copy()",
   "id": "51aa90d8e640ea87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Descriptive Analytics",
   "id": "c29154d4aca422a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Summary statistics\n",
    "print(\"Summary Statistics for Churned Customers:\")\n",
    "print(cleaned_set[cleaned_set['churn'] == 1].describe())\n",
    "print(\"\\nSummary Statistics for Non-Churned Customers:\")\n",
    "print(cleaned_set[cleaned_set['churn'] == 0].describe())\n",
    "\n",
    "# Distribution of categorical variables\n",
    "categorical_columns = ['gender', 'subscription_type', 'contract_length']\n",
    "for column in categorical_columns:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.countplot(data=cleaned_set, x=column, hue='churn')\n",
    "    plt.title(f'Distribution of {column} for Churned and Non-Churned Customers')\n",
    "    plt.show()\n",
    "\n",
    "# Correlation analysis\n",
    "correlation = cleaned_set.corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Churn rate\n",
    "churn_rate = cleaned_set['churn'].mean() * 100\n",
    "print(f\"Churn Rate: {churn_rate}%\")"
   ],
   "id": "94855a09b7c82e34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_cols = [\"age\", \"tenure\", \"usage_frequency\", \"support_calls\", \"payment_delay\", \"last_interaction\", \"total_spend\"]\n",
    "\n",
    "num_bins = 3\n",
    "\n",
    "excourse_set = cleaned_set.copy()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    excourse_set[col] = pd.cut(cleaned_set[col], num_bins, duplicates='drop')\n",
    "    print(col)\n",
    "    for interval in excourse_set[col].cat.categories:\n",
    "        print(interval)"
   ],
   "id": "a1670fa2ee6fbd71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns = [\"age\", \"gender\", \"tenure\", \"usage_frequency\", \"support_calls\", \"payment_delay\", \"subscription_type\", \"contract_length\", \"total_spend\", \"last_interaction\"]\n",
    "\n",
    "stacked_data_percent = {}\n",
    "\n",
    "for col in columns:\n",
    "    category_counts = excourse_set.groupby([col, \"churn\"]).size().unstack(fill_value=0)\n",
    "    \n",
    "    category_percent = category_counts.div(category_counts.sum(axis=1), axis=0) * 100\n",
    "    print(category_percent)\n",
    "    stacked_data_percent[col] = category_percent\n",
    "    "
   ],
   "id": "129bd3f3c68f5921",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the overall churn rate\n",
    "overall_churn_rate = excourse_set['churn'].mean() * 100\n",
    "\n",
    "# Add a new row to each DataFrame in stacked_data_percent with the overall churn rate\n",
    "for col, df in stacked_data_percent.items():\n",
    "    df.loc['Overall'] = [100 - overall_churn_rate, overall_churn_rate]\n",
    "    \n",
    "colors = {0: 'green', 1: 'red'}\n",
    "for col, df in stacked_data_percent.items():\n",
    "    ax = df.plot(kind='barh', stacked=True, color=[colors[churn] for churn in df.columns],\n",
    "                 title=f'Percentage Chart of Churned Customers in {col}')\n",
    "    plt.ylabel(col)\n",
    "    plt.xlabel('Percentage')\n",
    "    plt.legend([\"No Churn\", \"Churn\"], loc='best')\n",
    "\n",
    "    # Add the percentage values on each bar\n",
    "    for p in ax.patches:\n",
    "        width = p.get_width()\n",
    "        height = p.get_height()\n",
    "        x, y = p.get_xy()\n",
    "        ax.text(x+width/2,\n",
    "                y+height/2,\n",
    "                '{:.1f} %'.format(width),\n",
    "                horizontalalignment='center',\n",
    "                verticalalignment='center')\n",
    "    plt.show()"
   ],
   "id": "5765c52a8a7fe3a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "combinations = list(itertools.combinations(columns, 2))\n",
    "\n",
    "# Analyze each combination\n",
    "for combination in combinations:\n",
    "    # Create a multi-index DataFrame\n",
    "    multi_index_df = excourse_set.set_index(list(combination) + ['churn'])\n",
    "\n",
    "    # Calculate the size of each group\n",
    "    grouped_df = multi_index_df.groupby(list(combination) + ['churn']).size()\n",
    "\n",
    "    # Unstack the DataFrame to get a cross-tabulation\n",
    "    cross_tab = grouped_df.unstack(fill_value=0)\n",
    "\n",
    "    # Convert absolute numbers to relative percentages\n",
    "    cross_tab_percent = cross_tab.div(cross_tab.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # Print the cross-tabulation\n",
    "    print(f\"Cross-tabulation for {combination}:\")\n",
    "    print(cross_tab_percent)\n",
    "    print(\"\\n\")"
   ],
   "id": "5dc8f5611d13dc46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "combinations = list(itertools.combinations(columns, 3))\n",
    "\n",
    "# Analyze each combination\n",
    "for combination in combinations:\n",
    "    # Create a multi-index DataFrame\n",
    "    multi_index_df = excourse_set.set_index(list(combination) + ['churn'])\n",
    "\n",
    "    # Calculate the size of each group\n",
    "    grouped_df = multi_index_df.groupby(list(combination) + ['churn']).size()\n",
    "\n",
    "    # Unstack the DataFrame to get a cross-tabulation\n",
    "    cross_tab = grouped_df.unstack(fill_value=0)\n",
    "\n",
    "    # Convert absolute numbers to relative percentages\n",
    "    cross_tab_percent = cross_tab.div(cross_tab.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # Print the cross-tabulation\n",
    "    print(f\"Cross-tabulation for {combination}:\")\n",
    "    print(cross_tab_percent)\n",
    "    print(\"\\n\")"
   ],
   "id": "f7109f9988cac69e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Predictive Analytics\n",
    "## Primitive Approach"
   ],
   "id": "92fe04358e315dd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prepared_set = cleaned_set.copy()\n",
    "\n",
    "# Create a OneHotEncoder instance\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = ['gender', 'subscription_type', 'contract_length']\n",
    "\n",
    "# Fit and transform the data, converting it into a DataFrame\n",
    "prepared_set_encoded = pd.DataFrame(encoder.fit_transform(prepared_set[categorical_cols]))\n",
    "\n",
    "# Get feature names from the encoder and assign them as column names\n",
    "prepared_set_encoded.columns = encoder.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "prepared_set.drop(categorical_cols, axis=1, inplace=True)\n",
    "\n",
    "# Reset the indices of the dataframes\n",
    "prepared_set = prepared_set.reset_index(drop=True)\n",
    "prepared_set_encoded = prepared_set_encoded.reset_index(drop=True)\n",
    "\n",
    "# Concatenate the original DataFrame with the one-hot encoded DataFrame\n",
    "prepared_set = pd.concat([prepared_set, prepared_set_encoded], axis=1)\n",
    "\n",
    "prepared_set.info()"
   ],
   "id": "2c3a204f8b5fde19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "primitive_set = prepared_set.copy()\n",
    "\n",
    "X = primitive_set.drop('churn', axis=1)\n",
    "y = primitive_set['churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n"
   ],
   "id": "bfda46308fe5ba5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the performance metrics\n",
    "\n",
    "# Predict the probabilities of the positive class\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "primitive_mse = mean_squared_error(y_test, y_pred)\n",
    "primitive_mae = mean_absolute_error(y_test, y_pred)\n",
    "primitive_r2 = r2_score(y_test, y_pred)\n",
    "primitive_accuracy = accuracy_score(y_test, y_pred)\n",
    "primitive_precision = precision_score(y_test, y_pred)\n",
    "primitive_recall = recall_score(y_test, y_pred)\n",
    "primitive_f1 = f1_score(y_test, y_pred)\n",
    "primitive_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"MSE: {primitive_mse}\\nMAE: {primitive_mae}\\nR2: {primitive_r2}\\nAccuracy: {primitive_accuracy}\\nPrecision: {primitive_precision}\\nRecall: {primitive_recall}\\nF1 Score: {primitive_f1}\\nROC AUC: {primitive_roc_auc}\")"
   ],
   "id": "bef34fddc91311fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# My Model",
   "id": "9b3ba607804b7ef1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Define Target and Feauture Variables & Split and Scale Set\n",
    "Split the data into training, validation, and test sets, then, standardise the features"
   ],
   "id": "6a670e58e3acaca2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "better_model = prepared_set.copy()\n",
    "\n",
    "# Define Target and feature variables\n",
    "y = better_model['churn'].values\n",
    "X = better_model.drop(['churn'], axis=1)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Perform train-validation-test split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X[feature_names], y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.285, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_train_val = scaler.transform(X_train_val)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "id": "4d461c3142cb6327",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train and Evaluate Models\n",
    "Perform hyperparameter tuning for the LightGBM using grid search. Print the best hyperparameters and the corresponding R-squared(on subset of training set(Cross-Validation)) score."
   ],
   "id": "e4dcf8a1dde95d9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the parameter lgbm_grid\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'num_leaves': [31, 62, 93],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Create a LightGBM model\n",
    "lgbm_model = lgb.LGBMRegressor()\n",
    "\n",
    "# Create the lgbm_grid search object\n",
    "lgbm_grid = GridSearchCV(lgbm_model, param_grid, cv=5, scoring='r2')\n",
    "\n",
    "# Fit the lgbm_grid search object to the data\n",
    "lgbm_grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(lgbm_grid.best_params_)\n",
    "print(lgbm_grid.best_score_)"
   ],
   "id": "bee293cc8b8fd571",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Perform hyperparameter tuning for the Decision Tree model using grid search. Print the best hyperparameters and the corresponding R-squared(on subset of training set(Cross-Validation)) score",
   "id": "4660d9d9ff68a79e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the parameter tree_grid\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [1.0, 'sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Create a DecisionTreeRegressor model\n",
    "decTree_model = DecisionTreeRegressor()\n",
    "\n",
    "#Create the tree_grid search object\n",
    "tree_grid = GridSearchCV(decTree_model, param_grid, cv=5, scoring=\"r2\")\n",
    "\n",
    "# fit the tree_grid search object to the data\n",
    "tree_grid.fit(X_train, y_train)\n",
    "\n",
    "#Print the best parameters and the corresponding score\n",
    "print(tree_grid.best_params_)\n",
    "print(tree_grid.best_score_)"
   ],
   "id": "a86b237455604e46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the parameter xgb_grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'subsample': [0.5, 0.7, 1.0],\n",
    "    'colsample_bytree': [0.4, 0.7, 1.0]\n",
    "}\n",
    "print(\"check 1\")\n",
    "# Create model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "print(\"check 2\")\n",
    "\n",
    "# Create grid search object\n",
    "xgb_grid = GridSearchCV(xgb_model, param_grid, cv=5, scoring='r2')\n",
    "print(\"check 3\")\n",
    "\n",
    "# Fit the xgb_grid search object to the data\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "print(\"check 4\")\n",
    "#Print the best  parameters and the corresponding score\n",
    "print(xgb_grid.best_params_)\n",
    "print(xgb_grid.best_score_)\n",
    "print(\"check 5\")\n",
    "\n",
    "importance_scores = xgb_grid.feature_importances_\n",
    "print(\"check 6\")\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importance_scores\n",
    "})\n",
    "print(\"check 7\")\n",
    "\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "print(\"check 8\")\n",
    "\n",
    "importance_df.plot(kind='bar', x='Feature', y='Importance', title='Feature Importance', figsize=(15, 6))\n",
    "plt.ylabel('Importance Score')\n",
    "plt.show()"
   ],
   "id": "7985f26c75b9aea7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_tree = tree_grid.best_estimator_.predict(X_val)\n",
    "y_pred_lgbm = lgbm_grid.best_estimator_.predict(X_val)\n",
    "y_pred_xgb = xgb_grid.best_estimator_.predict(X_val)\n",
    "\n",
    "mse_tree = mean_squared_error(y_val, y_pred_tree)\n",
    "mae_tree = mean_absolute_error(y_val, y_pred_tree)\n",
    "r2_tree = r2_score(y_val, y_pred_tree)\n",
    "mse_lgbm = mean_squared_error(y_val, y_pred_lgbm)\n",
    "mae_lgbm = mean_absolute_error(y_val, y_pred_lgbm)\n",
    "r2_lgbm = r2_score(y_val, y_pred_lgbm)\n",
    "mse_xgb = mean_squared_error(y_val, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_val, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_val, y_pred_xgb)\n",
    "\n",
    "models = ['Decision Tree', 'LightGBM', 'XGBoost']\n",
    "mse_values = [mse_tree, mse_lgbm, mse_xgb]\n",
    "mae_values = [mae_tree, mae_lgbm, mae_xgb]\n",
    "r2_values = [r2_tree, r2_lgbm, r2_xgb]\n",
    "\n",
    "# Function to print the values\n",
    "def print_values(values, models, title):\n",
    "    print(title)\n",
    "    for model, value in zip(models, values):\n",
    "        print(f\"{model}: {value}\")\n",
    "\n",
    "# Function to plot the values\n",
    "def plot_values(values, title, models, ylabel):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    sns.barplot(x=models, y=values)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "# Print and plot MSE values\n",
    "print_values(mse_values, models, 'Comparison of Mean Squared Error')\n",
    "print_values(r2_values, models, 'Comparison of R2 Score')\n",
    "print_values(mae_values, models, 'Comparison of Mean Absolute Error')\n",
    "plot_values(mse_values, 'Comparison of Mean Squared Error', models, 'MSE')\n",
    "\n",
    "# Print and plot MAE values\n",
    "\n",
    "plot_values(mae_values, 'Comparison of Mean Absolute Error', models, 'MAE')\n",
    "\n",
    "# Print and plot R2 values\n",
    "\n",
    "plot_values(r2_values, 'Comparison of R2 Score', models, 'R2 Score')"
   ],
   "id": "d83524c15b94ea6c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
